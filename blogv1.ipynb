{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import requests\n",
    "import datetime\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base url which i can use as a base and add the dates to so i can access the different pages\n",
    "base_url = \"https://www.transfermarkt.co.uk/premier-league/marktwerteverein/wettbewerb/GB1/stichtag/\"\n",
    "\n",
    "# Dictionary to store DataFrames for each date\n",
    "dfs_prem_value = {}\n",
    "\n",
    "# Custom headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "}\n",
    "\n",
    "# Loop through each year from 2011 to 2024\n",
    "for year in range(2011, 2025):\n",
    "    date_str = f\"{year}-03-15\"\n",
    "    url = base_url + date_str \n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching page for {date_str}: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # First, try to find the table with class \"items\"\n",
    "    table = soup.find('table', class_=\"items\")\n",
    "    \n",
    "    # If not found, look inside HTML comments\n",
    "    if not table:\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "        for comment in comments:\n",
    "            comment_soup = BeautifulSoup(comment, \"html.parser\")\n",
    "            table = comment_soup.find('table', class_=\"items\")\n",
    "            if table:\n",
    "                break\n",
    "\n",
    "    if table:\n",
    "        try:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            dfs_prem_value[date_str] = df\n",
    "            print(f\"Found items table for {date_str} with {len(df)} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing table for {date_str}: {e}\")\n",
    "    else:\n",
    "        print(f\"No items table found for {date_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern_top6 matching the desired club names in lower-case\n",
    "pattern_top6 = r'manchester city|manchester united|chelsea fc|tottenham hotspur|liverpool fc|arsenal fc'\n",
    "pattern_bottom6 = r'southampton fc|leicester city|ipswich town|fulham fc|everton fc|wolverhampton wanderers'\n",
    "\n",
    "# Dictionary to store the filtered DataFrames\n",
    "filtered_dataframes_top_6 = {}\n",
    "\n",
    "# Loop through each date and DataFrame in the scraped data\n",
    "for date, df in dfs_prem_value.items():\n",
    "    # Check that the expected column is present\n",
    "    if 'Club' in df.columns:\n",
    "        # Filter rows by matching the pattern_top6 in a case-insensitive manner.\n",
    "        filtered_df_top = df[df['Club'].str.lower().str.contains(pattern_top6, na=False)]\n",
    "        filtered_dataframes_top_6[date] = filtered_df_top\n",
    "        print(f\"{date}: {len(filtered_df_top)} rows retained.\")\n",
    "    else:\n",
    "        print(f\"DataFrame for {date} does not contain a 'Club' column. Available columns: {df.columns}\")\n",
    "\n",
    "\n",
    "# Dictionary to store the filtered DataFrames\n",
    "filtered_dataframes_bottom_6 = {}\n",
    "\n",
    "# Loop through each date and DataFrame in your scraped data\n",
    "for date, df in dfs_prem_value.items():\n",
    "    # Check that the expected column is present\n",
    "    if 'Club' in df.columns:\n",
    "        # Filter rows by matching the pattern_top6 in a case-insensitive manner.\n",
    "        filtered_df_bottom = df[df['Club'].str.lower().str.contains(pattern_bottom6, na=False)]\n",
    "        filtered_dataframes_bottom_6[date] = filtered_df_bottom\n",
    "        print(f\"{date}: {len(filtered_df_bottom)} rows retained.\")\n",
    "    else:\n",
    "        print(f\"DataFrame for {date} does not contain a 'Club' column. Available columns: {df.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes_top_6['2022-03-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes_bottom_6['2022-03-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names to drop\n",
    "columns_to_drop = ['#', 'wappen', 'Club.1', 'Current value', '%', 'Unnamed: 8','Unnamed: 9' ]  # Replace with your actual column names\n",
    "\n",
    "# Loop through each DataFrame in your dictionary (e.g., filtered_dataframes)\n",
    "for date, df in filtered_dataframes_top_6.items():\n",
    "    # Drop the columns and update the DataFrame in the dictionary\n",
    "    # Using errors='ignore' ensures that if a column is missing, it won't raise an error\n",
    "    filtered_dataframes_top_6[date] = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "for date, df in filtered_dataframes_bottom_6.items():\n",
    "    # Drop the columns and update the DataFrame in the dictionary\n",
    "    # Using errors='ignore' ensures that if a column is missing, it won't raise an error\n",
    "    filtered_dataframes_bottom_6[date] = df.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dictionary of DataFrames is called 'filtered_dataframes'\n",
    "for date, df in filtered_dataframes_top_6.items():\n",
    "    # Check if the column 'League' exists and then rename it to the date\n",
    "    if 'League' in df.columns:\n",
    "        df.rename(columns={'League': \"Value_\" + date}, inplace=True)\n",
    "    else:\n",
    "        print(f\"'League' column not found in DataFrame for {date}\")\n",
    "\n",
    "for date, df in filtered_dataframes_bottom_6.items():\n",
    "    # Check if the column 'League' exists and then rename it to the date\n",
    "    if 'League' in df.columns:\n",
    "        df.rename(columns={'League': \"Value_\" + date}, inplace=True)\n",
    "    else:\n",
    "        print(f\"'League' column not found in DataFrame for {date}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes_top_6['2014-03-15'].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes_bottom_6['2014-03-15'].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dictionary to store the subset DataFrames\n",
    "filtered_dataframes_top_6_v1 = {}\n",
    "\n",
    "for date, df in filtered_dataframes_top_6.items():\n",
    "    # Construct the value column name based on the date\n",
    "    value_col = \"Value_\" + date\n",
    "    if 'Club' in df.columns and value_col in df.columns:\n",
    "        # Select only the 'Club' and the 'Value_(date)' columns\n",
    "        subset_df = df[['Club', value_col]].copy()\n",
    "        filtered_dataframes_top_6_v1[date] = subset_df\n",
    "        print(f\"For {date}: Retained columns: {subset_df.columns.tolist()}\")\n",
    "    else:\n",
    "        print(f\"DataFrame for {date} does not have the required columns: 'Club' and {value_col}\")\n",
    "\n",
    "filtered_dataframes_bottom_6_v1 = {}\n",
    "\n",
    "for date, df in filtered_dataframes_bottom_6.items():\n",
    "    # Construct the value column name based on the date\n",
    "    value_col = \"Value_\" + date\n",
    "    if 'Club' in df.columns and value_col in df.columns:\n",
    "        # Select only the 'Club' and the 'Value_(date)' columns\n",
    "        subset_df = df[['Club', value_col]].copy()\n",
    "        filtered_dataframes_bottom_6_v1[date] = subset_df\n",
    "        print(f\"For {date}: Retained columns: {subset_df.columns.tolist()}\")\n",
    "    else:\n",
    "        print(f\"DataFrame for {date} does not have the required columns: 'Club' and {value_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes_top_6_v1['2017-03-15'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes_bottom_6_v1['2017-03-15'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an empty combined dataframe\n",
    "combined_df_top6 = None\n",
    "combined_df_bottom6 = None\n",
    "\n",
    "# Loop through each date and merge on the 'Club' column\n",
    "for date, df in filtered_dataframes_top_6_v1.items():\n",
    "    if combined_df_top6 is None:\n",
    "        combined_df_top6 = df\n",
    "    else:\n",
    "        combined_df_top6 = pd.merge(combined_df_top6, df, on='Club', how='outer')\n",
    "\n",
    "# Display the first few rows of the combined dataframe\n",
    "print(combined_df_top6.head())\n",
    "\n",
    "for date, df in filtered_dataframes_bottom_6_v1.items():\n",
    "    if combined_df_bottom6 is None:\n",
    "        combined_df_bottom6 = df\n",
    "    else:    \n",
    "        combined_df_bottom6 = pd.merge(combined_df_bottom6, df, on='Club', how='outer') \n",
    "print(combined_df_bottom6.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_value(value_str):\n",
    "    \"\"\"\n",
    "    Convert a monetary string (e.g., \"€310.75m\", \"€1.19bn\") into a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value_str, str):\n",
    "        # Remove the euro symbol and extra spaces, then convert to lower case\n",
    "        value_str = value_str.replace(\"€\", \"\").strip().lower()\n",
    "        if \"m\" in value_str:\n",
    "            try:\n",
    "                # Remove \"m\", convert to float, and multiply by 1e6\n",
    "                return float(value_str.replace(\"m\", \"\")) * 1_000_000\n",
    "            except:\n",
    "                return None\n",
    "        elif \"bn\" in value_str:\n",
    "            try:\n",
    "                # Remove \"bn\", convert to float, and multiply by 1e9\n",
    "                return float(value_str.replace(\"bn\", \"\")) * 1_000_000_000\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return float(value_str)\n",
    "            except:\n",
    "                return None\n",
    "    return value_str\n",
    "\n",
    "# Assuming your combined dataframe is named 'combined_df_top6'\n",
    "# Loop through all columns and apply conversion on columns that start with \"Value_\"\n",
    "for col in combined_df_top6.columns:\n",
    "    if col.startswith(\"Value_\"):\n",
    "        combined_df_top6[col] = combined_df_top6[col].apply(convert_value)\n",
    "\n",
    "for col in combined_df_bottom6.columns:\n",
    "    if col.startswith(\"Value_\"):\n",
    "        combined_df_bottom6[col] = combined_df_bottom6[col].apply(convert_value)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transposed_top6 = combined_df_top6.set_index('Club').transpose()\n",
    "\n",
    "# Remove the \"Value_\" prefix from the index and convert to datetime objects.\n",
    "df_transposed_top6.index = pd.to_datetime(df_transposed_top6.index.str.replace(\"Value_\", \"\", regex=True))\n",
    "df_transposed_top6 = df_transposed_top6.sort_index()\n",
    "\n",
    "df_transposed_bottom6 = combined_df_bottom6.set_index('Club').transpose()\n",
    "\n",
    "# Remove the \"Value_\" prefix from the index and convert to datetime objects.\n",
    "df_transposed_bottom6.index = pd.to_datetime(df_transposed_bottom6.index.str.replace(\"Value_\", \"\", regex=True))\n",
    "df_transposed_bottom6 = df_transposed_bottom6.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Custom Y-Axis Formatter ---------------------- #\n",
    "def custom_y_formatter(x, pos):\n",
    "    if x < 1e9:\n",
    "        return f\"{x/1e6:,.0f} million\"\n",
    "    else:\n",
    "        return f\"{x/1e9:,.1f} billion\"\n",
    "\n",
    "# Set Seaborn style and palette for aesthetics.\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Create a figure with 2 subplots (vertical layout).\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# ---------------------- Plotting Function ---------------------- #\n",
    "def plot_data(ax, df_transposed, title):\n",
    "    # Convert datetime index to numeric for spline interpolation.\n",
    "    x_dates = mdates.date2num(df_transposed.index.to_pydatetime())\n",
    "    \n",
    "    # Plot each club's data.\n",
    "    for club in df_transposed.columns:\n",
    "        y = df_transposed[club].values\n",
    "        if len(x_dates) >= 3:\n",
    "            spline = make_interp_spline(x_dates, y, k=3)  # Cubic spline for smoothness.\n",
    "            x_dense = np.linspace(x_dates.min(), x_dates.max(), 300)\n",
    "            y_smooth = spline(x_dense)\n",
    "            x_dense_dates = mdates.num2date(x_dense)\n",
    "            ax.plot(x_dense_dates, y_smooth, label=club, linewidth=2)\n",
    "        else:\n",
    "            ax.plot(df_transposed.index, y, marker='o', label=club, linewidth=2)\n",
    "    \n",
    "    # Format the x-axis: one tick per year, display only the year.\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Disable scientific notation/offsets on the y-axis.\n",
    "    ax.ticklabel_format(axis='y', style='plain', useOffset=False)\n",
    "    ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "    \n",
    "    # Limit the number of y-axis ticks.\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(6))\n",
    "    \n",
    "    # Apply the custom y-axis formatter.\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(custom_y_formatter))\n",
    "    \n",
    "    # Set titles and labels.\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Investment Value\", fontsize=12)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0)\n",
    "\n",
    "# ---------------------- Plot for Top 6 Teams ---------------------- #\n",
    "plot_data(axes[0], df_transposed_top6, \"Top 6 Teams Values Over Time (2011-2024)\")\n",
    "\n",
    "# ---------------------- Plot for Bottom 6 Teams ---------------------- #\n",
    "plot_data(axes[1], df_transposed_bottom6, \"Bottom 6 Teams Values Over Time (2011-2024)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how this compares to the top 5 leagues in eurpope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping leagues to their base URL\n",
    "base_urls = {\n",
    "    \"La Liga\": \"https://www.transfermarkt.co.uk/la-liga/marktwerteverein/wettbewerb/ES1/stichtag/\",\n",
    "    \"Bundesliga\": \"https://www.transfermarkt.co.uk/bundesliga/marktwerteverein/wettbewerb/L1/stichtag/\",\n",
    "    \"Ligue 1\": \"https://www.transfermarkt.co.uk/ligue-1/marktwerteverein/wettbewerb/FR1/stichtag/\",\n",
    "    \"Serie A\": \"https://www.transfermarkt.co.uk/serie-a/marktwerteverein/wettbewerb/IT1/stichtag/\",\n",
    "    \"Premier League\": \"https://www.transfermarkt.co.uk/premier-league/marktwerteverein/wettbewerb/GB1/stichtag/\"\n",
    "}\n",
    "\n",
    "# Dictionary to store DataFrames for each league and date\n",
    "dfs_league = {}\n",
    "\n",
    "# Custom headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "}\n",
    "\n",
    "# Loop through each league\n",
    "for league, base_url in base_urls.items():\n",
    "    dfs_league[league] = {}  # create a sub-dictionary for this league\n",
    "    print(f\"\\nScraping data for {league}:\")\n",
    "    \n",
    "    # Loop through each year from 2011 to 2024\n",
    "    for year in range(2011, 2025):\n",
    "        date_str = f\"{year}-03-15\"\n",
    "        url = base_url + date_str\n",
    "        print(f\"Scraping: {url}\")\n",
    "    \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching page for {league} {date_str}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        # Try to find the table with class \"items\"\n",
    "        table = soup.find('table', class_=\"items\")\n",
    "    \n",
    "        # If not found, look inside HTML comments\n",
    "        if not table:\n",
    "            comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "            for comment in comments:\n",
    "                comment_soup = BeautifulSoup(comment, \"html.parser\")\n",
    "                table = comment_soup.find('table', class_=\"items\")\n",
    "                if table:\n",
    "                    break\n",
    "\n",
    "        if table:\n",
    "            try:\n",
    "                df = pd.read_html(str(table))[0]\n",
    "                dfs_league[league][date_str] = df\n",
    "                print(f\"Found items table for {league} {date_str} with {len(df)} rows.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing table for {league} {date_str}: {e}\")\n",
    "        else:\n",
    "            print(f\"No items table found for {league} {date_str}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names to drop\n",
    "columns_to_drop = ['#', 'wappen', 'Club.1', 'Current value', '%', 'Unnamed: 8','Unnamed: 9' ]  # Replace with your actual column names\n",
    "\n",
    "# Loop through each league and its corresponding DataFrames in the nested dictionary\n",
    "for league, league_data in dfs_league.items():\n",
    "    for date, df in league_data.items():\n",
    "        # Drop the columns and update the DataFrame in the nested dictionary\n",
    "        # Using errors='ignore' ensures that if a column is missing, it won't raise an error\n",
    "        dfs_league[league][date] = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each league and its corresponding DataFrames in the nested dictionary\n",
    "for league, league_data in dfs_league.items():\n",
    "    for date, df in league_data.items():\n",
    "        # Check if the column 'League' exists and then rename it to the date\n",
    "        if 'League' in df.columns:\n",
    "            df.rename(columns={'League': \"Value_\" + date}, inplace=True)\n",
    "        else:\n",
    "            print(f\"'League' column not found in DataFrame for {league} on {date}\")\n",
    "\n",
    "# New dictionary to store the subset DataFrames\n",
    "dfs_league_cleaned = {}\n",
    "\n",
    "for league, league_data in dfs_league.items():\n",
    "    for date, df in league_data.items():\n",
    "        # Construct the value column name based on the date\n",
    "        value_col = \"Value_\" + date\n",
    "        if 'Club' in df.columns and value_col in df.columns:\n",
    "            # Select only the 'Club' and the 'Value_(date)' columns\n",
    "            subset_df = df[['Club', value_col]].copy()\n",
    "            if league not in dfs_league_cleaned:\n",
    "                dfs_league_cleaned[league] = {}\n",
    "            dfs_league_cleaned[league][date] = subset_df\n",
    "\n",
    "# Start with an empty combined dataframe\n",
    "\n",
    "combined_la_liga = None\n",
    "combined_ligue_1 = None\n",
    "combined_bundesliga = None\n",
    "combined_serie_a = None\n",
    "combined_premier_league = None\n",
    "\n",
    "# Loop through each league and its corresponding DataFrames in the nested dictionary  \n",
    "for league, league_data in dfs_league_cleaned.items():\n",
    "    for date, df in league_data.items():\n",
    "        if league == 'La Liga':\n",
    "            if combined_la_liga is None:\n",
    "                combined_la_liga = df\n",
    "            else:\n",
    "                combined_la_liga = pd.merge(combined_la_liga, df, on='Club', how='outer')\n",
    "        elif league == 'Ligue 1':\n",
    "            if combined_ligue_1 is None:\n",
    "                combined_ligue_1 = df\n",
    "            else:\n",
    "                combined_ligue_1 = pd.merge(combined_ligue_1, df, on='Club', how='outer')\n",
    "        elif league == 'Bundesliga':\n",
    "            if combined_bundesliga is None:\n",
    "                combined_bundesliga = df\n",
    "            else:\n",
    "                combined_bundesliga = pd.merge(combined_bundesliga, df, on='Club', how='outer')\n",
    "        elif league == 'Serie A':\n",
    "            if combined_serie_a is None:\n",
    "                combined_serie_a = df\n",
    "            else:\n",
    "                combined_serie_a = pd.merge(combined_serie_a, df, on='Club', how='outer')\n",
    "        elif league == 'Premier League':\n",
    "            if combined_premier_league is None:\n",
    "                combined_premier_league = df\n",
    "            else:\n",
    "                combined_premier_league = pd.merge(combined_premier_league, df, on='Club', how='outer')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# realised that there was a unknown row in the combined dataframe that was causing issues with the plotting\n",
    "combined_ligue_1_clean = combined_ligue_1.drop(18)\n",
    "combined_la_liga_clean = combined_la_liga.drop(17)\n",
    "combined_bundesliga_clean = combined_bundesliga.drop(15)\n",
    "combined_serie_a_clean = combined_serie_a.drop(17)\n",
    "combined_premier_league_clean = combined_premier_league.drop(17)\n",
    "\n",
    "\n",
    "def convert_value(value_str):\n",
    "    \"\"\"\n",
    "    Convert a monetary string (e.g., \"€310.75m\", \"€1.19bn\") into a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value_str, str):\n",
    "        # Remove the euro symbol and extra spaces, then convert to lower case\n",
    "        value_str = value_str.replace(\"€\", \"\").strip().lower()\n",
    "        if \"m\" in value_str:\n",
    "            try:\n",
    "                # Remove \"m\", convert to float, and multiply by 1e6\n",
    "                return float(value_str.replace(\"m\", \"\")) * 1_000_000\n",
    "            except:\n",
    "                return None\n",
    "        elif \"bn\" in value_str:\n",
    "            try:\n",
    "                # Remove \"bn\", convert to float, and multiply by 1e9\n",
    "                return float(value_str.replace(\"bn\", \"\")) * 1_000_000_000\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return float(value_str)\n",
    "            except:\n",
    "                return None\n",
    "    return value_str\n",
    "\n",
    "# List of league DataFrames\n",
    "league_dfs1 = [\n",
    "    combined_ligue_1_clean,\n",
    "    combined_la_liga_clean,\n",
    "    combined_bundesliga_clean,\n",
    "    combined_serie_a_clean,\n",
    "    combined_premier_league_clean\n",
    "]\n",
    "\n",
    "# Loop through each league DataFrame and convert the columns starting with \"Value_\"\n",
    "for df in league_dfs1:\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"Value_\"):\n",
    "            df[col] = df[col].apply(convert_value)\n",
    "\n",
    "\n",
    "league_dfs1[0] = combined_ligue_1_clean\n",
    "league_dfs1[1] = combined_la_liga_clean\n",
    "league_dfs1[2] = combined_bundesliga_clean\n",
    "league_dfs1[3] = combined_serie_a_clean\n",
    "league_dfs1[4] = combined_premier_league_clean\n",
    "\n",
    "# Transpose and prepare data for Ligue 1\n",
    "df_transposed_ligue_1 = combined_ligue_1_clean.set_index('Club').transpose()\n",
    "df_transposed_ligue_1.index = pd.to_datetime(df_transposed_ligue_1.index.str.replace(\"Value_\", \"\", regex=True))\n",
    "df_transposed_ligue_1 = df_transposed_ligue_1.sort_index()\n",
    "\n",
    "# Transpose and prepare data for La Liga\n",
    "df_transposed_la_liga = combined_la_liga_clean.set_index('Club').transpose()\n",
    "df_transposed_la_liga.index = pd.to_datetime(df_transposed_la_liga.index.str.replace(\"Value_\", \"\", regex=True))\n",
    "df_transposed_la_liga = df_transposed_la_liga.sort_index()\n",
    "\n",
    "# Transpose and prepare data for Bundesliga\n",
    "df_transposed_bundesliga = combined_bundesliga_clean.set_index('Club').transpose()\n",
    "df_transposed_bundesliga.index = pd.to_datetime(df_transposed_bundesliga.index.str.replace(\"Value_\", \"\", regex=True))\n",
    "df_transposed_bundesliga = df_transposed_bundesliga.sort_index()\n",
    "\n",
    "# Transpose and prepare data for Serie A\n",
    "df_transposed_serie_a = combined_serie_a_clean.set_index('Club').transpose()\n",
    "df_transposed_serie_a.index = pd.to_datetime(df_transposed_serie_a.index.str.replace(\"Value_\", \"\", regex=True))\n",
    "df_transposed_serie_a = df_transposed_serie_a.sort_index()\n",
    "\n",
    "# Transpose and prepare data for Premier League\n",
    "df_transposed_premier_league = combined_premier_league_clean.set_index('Club').transpose()\n",
    "df_transposed_premier_league.index = pd.to_datetime(df_transposed_premier_league.index.str.replace(\"Value_\", \"\", regex=True))\n",
    "df_transposed_premier_league = df_transposed_premier_league.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Custom Y-Axis Formatter ---------------------- #\n",
    "def custom_y_formatter(x, pos):\n",
    "    if x < 1e9:\n",
    "        return f\"{x/1e6:,.0f} million\"\n",
    "    else:\n",
    "        return f\"{x/1e9:,.1f} billion\"\n",
    "\n",
    "# ---------------------- Set Seaborn Style ---------------------- #\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# ---------------------- Function to Plot League Average ---------------------- #\n",
    "def plot_league_avg(ax, df, label):\n",
    "    # Compute the average value across all clubs for each date (index)\n",
    "    avg_series = df.mean(axis=1)\n",
    "    # Convert the datetime index to numeric values for spline interpolation.\n",
    "    x_dates = mdates.date2num(avg_series.index.to_pydatetime())\n",
    "    y = avg_series.values\n",
    "    \n",
    "    # If there are at least 3 points, use cubic spline interpolation for a smooth curve.\n",
    "    if len(x_dates) >= 3:\n",
    "        spline = make_interp_spline(x_dates, y, k=3)\n",
    "        x_dense = np.linspace(x_dates.min(), x_dates.max(), 300)\n",
    "        y_smooth = spline(x_dense)\n",
    "        x_dense_dates = mdates.num2date(x_dense)\n",
    "        ax.plot(x_dense_dates, y_smooth, label=label, linewidth=2)\n",
    "    else:\n",
    "        ax.plot(avg_series.index, y, marker='o', label=label, linewidth=2)\n",
    "\n",
    "# ---------------------- Create Figure and Axis ---------------------- #\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# ---------------------- Plot the Average for Each League ---------------------- #\n",
    "plot_league_avg(ax, df_transposed_ligue_1, \"Ligue 1\")\n",
    "plot_league_avg(ax, df_transposed_la_liga, \"La Liga\")\n",
    "plot_league_avg(ax, df_transposed_bundesliga, \"Bundesliga\")\n",
    "plot_league_avg(ax, df_transposed_serie_a, \"Serie A\")\n",
    "plot_league_avg(ax, df_transposed_premier_league, \"Premier League\")\n",
    "\n",
    "# ---------------------- Highlight Significant Financial Events ---------------------- #\n",
    "# Define events with their year and label. Adjust these events as needed.\n",
    "events = [\n",
    "    {\"year\": 2011, \"label\": \"UEFA FFP introduced\"},\n",
    "    {\"year\": 2020, \"label\": \"COVID-19 Pandemic\"}\n",
    "]\n",
    "\n",
    "for event in events:\n",
    "    # Define the start and end of the event year.\n",
    "    start_date = datetime.datetime(event['year'], 1, 1)\n",
    "    end_date = datetime.datetime(event['year'], 12, 31)\n",
    "    # Shade the entire year with a semi-transparent gray.\n",
    "    ax.axvspan(start_date, end_date, color='gray', alpha=0.2)\n",
    "    # Add a vertical annotation for the event.\n",
    "    ax.text(start_date, ax.get_ylim()[1]*0.95, event['label'], rotation=90,\n",
    "            verticalalignment='top', fontsize=10, color='black')\n",
    "\n",
    "# ---------------------- Format X-Axis ---------------------- #\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ---------------------- Format Y-Axis ---------------------- #\n",
    "ax.ticklabel_format(axis='y', style='plain', useOffset=False)\n",
    "ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(6))\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(custom_y_formatter))\n",
    "\n",
    "# ---------------------- Set Title, Labels, and Legend ---------------------- #\n",
    "ax.set_title(\"Average of Club's values in Europes top 5 leagues (2011-2024)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Year\", fontsize=12)\n",
    "ax.set_ylabel(\"Investment Value\", fontsize=12)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "# Dictionary mapping leagues to their revenue/expenditure URL\n",
    "base_urls = {\n",
    "    \"Premier League\": \"https://www.transfermarkt.co.uk/premier-league/einnahmenausgaben/wettbewerb/GB1/plus/0?ids=a&sa=&saison_id=&saison_id_bis=2024&nat=&pos=&altersklasse=&w_s=&leihe=&intern=0\",\n",
    "    \"La Liga\": \"https://www.transfermarkt.co.uk/laliga/einnahmenausgaben/wettbewerb/ES1/plus/0?ids=a&sa=&saison_id=&saison_id_bis=2024&nat=&pos=&altersklasse=&w_s=&leihe=&intern=0\",\n",
    "    \"Serie A\": \"https://www.transfermarkt.co.uk/serie-a/einnahmenausgaben/wettbewerb/IT1/plus/0?ids=a&sa=&saison_id=&saison_id_bis=2024&nat=&pos=&altersklasse=&w_s=&leihe=&intern=0\",\n",
    "    \"Bundesliga\": \"https://www.transfermarkt.co.uk/bundesliga/einnahmenausgaben/wettbewerb/L1/plus/0?ids=a&sa=&saison_id=&saison_id_bis=2024&nat=&pos=&altersklasse=&w_s=&leihe=&intern=0\",\n",
    "    \"Ligue 1\": \"https://www.transfermarkt.co.uk/ligue-1/einnahmenausgaben/wettbewerb/FR1/plus/0?ids=a&sa=&saison_id=&saison_id_bis=2024&nat=&pos=&altersklasse=&w_s=&leihe=&intern=0\"\n",
    "}\n",
    "\n",
    "# Dictionary to store DataFrames for each league\n",
    "dfs_income_vs_expenditure = {}\n",
    "\n",
    "# Custom headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "}\n",
    "\n",
    "# Loop through each league URL and scrape the table\n",
    "for league, url in base_urls.items():\n",
    "    print(f\"\\nScraping data for {league}:\")\n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching page for {league}: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Try to find the table with class \"items\"\n",
    "    table = soup.find('table', class_=\"items\")\n",
    "    \n",
    "    # If not found, look inside HTML comments\n",
    "    if not table:\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "        for comment in comments:\n",
    "            comment_soup = BeautifulSoup(comment, \"html.parser\")\n",
    "            table = comment_soup.find('table', class_=\"items\")\n",
    "            if table:\n",
    "                break\n",
    "\n",
    "    if table:\n",
    "        try:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            dfs_income_vs_expenditure[league] = df\n",
    "            print(f\"Found items table for {league} with {len(df)} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing table for {league}: {e}\")\n",
    "    else:\n",
    "        print(f\"No items table found for {league}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_income_vs_expenditure['Premier League'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names to drop\n",
    "columns_to_drop = ['#', 'Club', 'Arrivals', 'Income', 'Departures','Balance' ]\n",
    "\n",
    "# Loop through each league and its corresponding DataFrames in the nested dictionary\n",
    "for league, df in dfs_income_vs_expenditure.items():\n",
    "        # Drop the columns and update the DataFrame in the nested dictionary\n",
    "        # Using errors='ignore' ensures that if a column is missing, it won't raise an error\n",
    "        dfs_income_vs_expenditure[league] = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_income_vs_expenditure['Ligue 1'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each league and its corresponding DataFrames in the nested dictionary\n",
    "for league, df in dfs_income_vs_expenditure.items():\n",
    "        # Check if the column 'Club.1' exists and then rename it to the date\n",
    "        if 'Club.1' in df.columns:\n",
    "            df.rename(columns={'Club.1': \"Club\"}, inplace=True)\n",
    "        if 'Club.2' in df.columns:\n",
    "            df.rename(columns={'Club.2': \"Club's Expenditure\"}, inplace=True)\n",
    "        if 'Expenditure' in df.columns:\n",
    "            df.rename(columns={'Expenditure': \"Arrivals\"}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the first 15 rows in each league df\n",
    "# Iterate through each DataFrame in the dictionary and slice it\n",
    "for league, df in dfs_income_vs_expenditure.items():\n",
    "\tdfs_income_vs_expenditure[league] = df.iloc[:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Club's Expenditure\" column to numeric values\n",
    "for league, df in dfs_income_vs_expenditure.items():\n",
    "    dfs_income_vs_expenditure[league][\"Club's Expenditure\"] = dfs_income_vs_expenditure[league][\"Club's Expenditure\"].apply(convert_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_income_vs_expenditure['La Liga'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Compute Average Expenditure ---------------------- #\n",
    "league_avg_expenditure = {}\n",
    "for league, df in dfs_income_vs_expenditure.items():\n",
    "    if \"Club's Expenditure\" in df.columns:\n",
    "        # Compute the mean expenditure for each league (ignoring missing values)\n",
    "        league_avg_expenditure[league] = df[\"Club's Expenditure\"].mean()\n",
    "\n",
    "df_avg = pd.DataFrame(\n",
    "    list(league_avg_expenditure.items()),\n",
    "    columns=[\"League\", \"AvgExpenditure\"]\n",
    ")\n",
    "df_avg.sort_values(\"AvgExpenditure\", inplace=True)\n",
    "\n",
    "# ---------------------- Compute Average Arrivals ---------------------- #\n",
    "league_avg_arrivals = {}\n",
    "for league, df in dfs_income_vs_expenditure.items():\n",
    "    if \"Arrivals\" in df.columns:\n",
    "        # Compute the mean arrivals for each league (ignoring missing values)\n",
    "        league_avg_arrivals[league] = df[\"Arrivals\"].mean()\n",
    "\n",
    "df_arrivals = pd.DataFrame(\n",
    "    list(league_avg_arrivals.items()),\n",
    "    columns=[\"League\", \"AvgArrivals\"]\n",
    ")\n",
    "# Ensure the order of arrivals matches expenditure (using League as key)\n",
    "df_arrivals = df_arrivals.set_index(\"League\").loc[df_avg[\"League\"]].reset_index()\n",
    "\n",
    "# ---------------------- Plotting the Bar Chart ---------------------- #\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=\"League\", y=\"AvgExpenditure\", data=df_avg, palette=\"Set2\")\n",
    "\n",
    "ax.set_title(\"Average Club Expenditure by League with Average Arrivals\", fontsize=16)\n",
    "ax.set_xlabel(\"League\", fontsize=14)\n",
    "ax.set_ylabel(\"Average Expenditure (€)\", fontsize=14)\n",
    "\n",
    "# ---------------------- Custom Y-Axis Formatter for Expenditure ---------------------- #\n",
    "def custom_y_formatter(x, pos):\n",
    "    if x < 1e9:\n",
    "        return f\"{x/1e6:,.0f}M\"\n",
    "    else:\n",
    "        return f\"{x/1e9:,.1f}B\"\n",
    "\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(custom_y_formatter))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate each bar with its expenditure value\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    label = custom_y_formatter(height, 0)\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,\n",
    "        height - (height * 0.1),\n",
    "        label,\n",
    "        ha='center',\n",
    "        va='top',\n",
    "        fontsize=12,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# ---------------------- Overlay Smooth Line Plot for Average Arrivals ---------------------- #\n",
    "# Create a secondary y-axis for arrivals\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"Average Arrivals (Count)\", fontsize=14, color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "ax2.grid(False)  # Disable gridlines on the secondary axis\n",
    "\n",
    "# Convert categorical league names to numeric positions\n",
    "leagues = df_avg[\"League\"].tolist()\n",
    "x_numeric = np.arange(len(leagues))\n",
    "y_arrivals = df_arrivals[\"AvgArrivals\"].values\n",
    "\n",
    "# Create a smooth (soft) line using spline interpolation if possible\n",
    "if len(x_numeric) >= 3:\n",
    "    x_dense = np.linspace(x_numeric.min(), x_numeric.max(), 300)\n",
    "    spline = make_interp_spline(x_numeric, y_arrivals, k=2)  # quadratic spline for smoothing\n",
    "    y_dense = spline(x_dense)\n",
    "    ax2.plot(x_dense, y_dense, color='blue', linewidth=1.5, label=\"Avg Arrivals\")\n",
    "else:\n",
    "    ax2.plot(x_numeric, y_arrivals, color='blue', marker='o', linewidth=1.5, label=\"Avg Arrivals\")\n",
    "\n",
    "# Overlay the original data points\n",
    "ax2.scatter(x_numeric, y_arrivals, color='black', marker='o', s=40)\n",
    "\n",
    "# Set x-ticks on the secondary axis to match the league names\n",
    "ax2.set_xticks(x_numeric)\n",
    "ax2.set_xticklabels(leagues)\n",
    "\n",
    "# Compute a vertical offset (3% of the y-axis range) to shift the text upward\n",
    "offset = (ax2.get_ylim()[1] - ax2.get_ylim()[0]) * 0.03\n",
    "\n",
    "# Annotate each point on the arrivals line with its value (shifted upward)\n",
    "# Also, shift the annotation horizontally for Premier League\n",
    "for x_val, y_val, league in zip(x_numeric, y_arrivals, leagues):\n",
    "    x_offset = 0.0\n",
    "    if league == \"Premier League\":\n",
    "        x_offset = 0.15  # Adjust this value as needed\n",
    "    ax2.text(x_val + x_offset, y_val + offset, f\"{y_val:.1f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Add a legend for the arrivals line (optional)\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
